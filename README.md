# 🔍 Spark ETL Project: Bank Churn Analysis

This project implements an **ETL (Extract, Transform, Load)** pipeline using **Apache Spark (PySpark)** to process and transform a dataset of bank customers. The objective is to prepare clean data suitable for analysis of **customer churn**.

---

## 📁 Project Structure

spark_etl_project/
├── data/
│ └── Bank_Churn.csv # Raw dataset
├── output/
│ └── transformed_data.csv # Transformed output
├── etl_spark.py # PySpark ETL script
├── requirements.txt # Python dependencies
├── .gitignore # Git ignore file
└── README.md # This file

## ⚙️ Features

- ✅ Built with **PySpark 4.0.0**
- ✅ Uses **Spark DataFrame API** for transformation
- ✅ Clean separation of raw and transformed data
- ✅ Output saved as `.csv`
- ✅ Ready for extension into Docker, Airflow, or AWS S3



## 🚀 How to Run

### 1. Clone the Repository

```bash
git clone https://github.com/Codewithkenny/spark_etl_project_Bank_Churn.git
cd spark_etl_project_Bank_Churn

📦 Requirements

    Python 3.8+

    Java 8 or 11 (for Spark)

    PySpark 4.0.0

Install all dependencies:

📫 Contact

Feel free to reach out:

Olapoju Agbomeji
GitHub: @Codewithkenny

📝 License

MIT License. See LICENSE for details.
